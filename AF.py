# -*- coding: utf-8 -*-
"""AF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1okt79vmHh-7L3sTr597T3fcbsWIIbf6U
"""

import numpy as np

"""
Activation functions
"""

import numpy as np

class Activation:
    def __init__(self, activation_function='sigmoid'):
        self.activation_function = activation_function

    def value(self, x):
        if self.activation_function == 'sigmoid':
            return self.sigmoid_value(x)
        elif self.activation_function == 'tanh':
            return self.tanh_value(x)
        elif self.activation_function == 'identity':
            return self.identity_value(x)
        elif self.activation_function == 'ReLU':
            return self.ReLU_value(x)
        elif self.activation_function == 'softmax':
            return self.softmax_value(x)
        else:
            raise ValueError("Unknown activation function")

    def grad(self, x):
        if self.activation_function == 'sigmoid':
            return self.sigmoid_grad(x)
        elif self.activation_function == 'tanh':
            return self.tanh_grad(x)
        elif self.activation_function == 'identity':
            return self.identity_grad(x)
        elif self.activation_function == 'ReLU':
            return self.ReLU_grad(x)
        elif self.activation_function == 'softmax':
            return self.softmax_grad(x)
        else:
            raise ValueError("Unknown activation function")

    def sigmoid_value(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_grad(self, x):
        return self.sigmoid_value(x) * (1 - self.sigmoid_value(x))

    def tanh_value(self, x):
        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

    def tanh_grad(self, x):
        return 1 - self.tanh_value(x)**2

    def identity_value(self, x):
        return x

    def identity_grad(self, x):
        return 1

    def ReLU_value(self, x):
        return np.maximum(0, x)

    def ReLU_grad(self, x):
        return np.where(x > 0, 1, 0)

    def softmax_value(self, x):
        return np.exp(x) / np.sum(np.exp(x), axis=0)

    def softmax_grad(self, x):
        return self.softmax_value(x) * (1 - self.softmax_value(x))


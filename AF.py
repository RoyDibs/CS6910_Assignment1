# -*- coding: utf-8 -*-
"""Activation functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16bObnp1cDyi41R-hakrq_xtlPNOhM8j7
"""

import numpy as np

"""
Activation functions
"""

class Sigmoid():

    def __init__(self):
        pass

    def value(self, x):
      return 1/(1 + np.exp(-x))

    def grad(self, x):
      return self.value(x)*(1 - self.value(x))

class tanh():

    def __init__(self):
        pass

    def value(self, x):
      return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

    def grad(self, x):
      return 1 - self.value(x)**2

class identity():

    def __init__(self):
        pass

    def value(self, x):
      return x

    def grad(self, x):
      return 1

class ReLU():

    def __init__(self):
        pass

    def value(self, x):
      return np.maximum(0, x)

    def grad(self, x):
      return np.where(x > 0, 1, 0)

class softmax():

    def __init__(self):
        pass

    def value(self, x):
        x_stable = x - np.max(x, axis=0, keepdims=True)
        exp_values = np.exp(x_stable)
        softmax_values = exp_values / np.sum(exp_values, axis=0, keepdims=True)
        return softmax_values
    # def value(self, x):
    #   return np.exp(x) / np.sum(np.exp(x), axis=0)

    def grad(self, x):
      return self.value(x) * (1 - self.value(x))


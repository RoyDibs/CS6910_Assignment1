# -*- coding: utf-8 -*-
"""optimiser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p8eWLAN_0wmV5HkGGrdvuirYTNBPgnPq
"""

import numpy as np

class Optimiser:
    def __init__(self):
        pass

    def stochastic_gradient_descent(self, parameters, grads, learning_rate):
        updated_parameters = parameters.copy()

        for param_name in parameters.keys():
            grad_key = 'd' + param_name

            updated_parameters[param_name] -= learning_rate * grads[grad_key]

        return updated_parameters


    def momentum(self, parameters, grads, velocities, beta, learning_rate):
        updated_parameters = parameters.copy()

        for param_name in parameters.keys():
            grad_key = 'd' + param_name

            # Update the velocities
            velocities[param_name] = beta * velocities[param_name] + learning_rate * grads[grad_key]

            # Update the parameters using the updated velocities
            updated_parameters[param_name] -= velocities[param_name]

        return updated_parameters


    def nestrov(self, X_batch, y_batch, parameters, velocities, beta, learning_rate, loss_function, activation_fn):
        updated_parameters = parameters.copy()  # Make a copy of the original parameters
        future_parameters = {}

        for param_name in parameters.keys():
            grad_key = 'd' + param_name
            # Update the velocities based on the gradients evaluated at the approximate future position
            future_parameters[param_name] = parameters[param_name] - beta * velocities[param_name]

        # Forward propagation
        AL, caches = forward_propagator.forward_propagation(X_batch, layer_dims, future_parameters, activation_fn)
        # Backward propagation
        grads = backpropagator.backward_propagation(y_batch, AL, caches, loss_function)

        for param_name in parameters.keys():
            grad_key = 'd' + param_name
            future_grads = grads[grad_key]  # Gradients evaluated at the approximate future position
            velocities[param_name] = beta * velocities[param_name] + learning_rate * future_grads
            # Update the parameters using the updated velocities
            updated_parameters[param_name] -= velocities[param_name]

        return updated_parameters


    def rmsprop(self, parameters, grads, velocities, beta, learning_rate):
        updated_parameters = parameters.copy()
        epsilon = 1e-4

        for param_name in parameters.keys():
            grad_key = 'd' + param_name

            # Update the velocities
            velocities[param_name] = beta * velocities[param_name] + (1 - beta) * grads[grad_key]**2

            # Update the parameters using the updated velocities
            updated_parameters[param_name] -= (learning_rate/np.sqrt(velocities[param_name] + epsilon)) * grads[grad_key]

        return updated_parameters


    def adam(self, parameters, grads, moment, velocities, beta1, beta2, learning_rate):
      updated_parameters = parameters.copy()
      epsilon = 1e-4
      moment_hat = {}
      velocities_hat = {}

      for param_name in parameters.keys():
          grad_key = 'd' + param_name

          # Update the moment
          moment[param_name] = beta1 * moment[param_name] + (1 - beta1) * grads[grad_key]

          moment_hat[param_name] = moment[param_name] / (1 - beta1)

          # Update the velocities
          velocities[param_name] = beta2 * velocities[param_name] + (1 - beta2) * grads[grad_key]**2

          velocities_hat[param_name] = velocities[param_name] / (1 - beta2)

          # Update the parameters using the updated velocities
          updated_parameters[param_name] -= (learning_rate/np.sqrt(velocities[param_name] + epsilon)) * moment_hat[param_name]

      return updated_parameters


    def nadam(self, parameters, grads, moment, velocities, beta1, beta2, learning_rate):
      updated_parameters = parameters.copy()
      epsilon = 1e-4
      moment_hat = {}
      velocities_hat = {}

      for param_name in parameters.keys():
          grad_key = 'd' + param_name

          # Update the moment
          moment[param_name] = beta1 * moment[param_name] + (1 - beta1) * grads[grad_key]

          moment_hat[param_name] = moment[param_name] / (1 - beta1)

          # Update the velocities
          velocities[param_name] = beta2 * velocities[param_name] + (1 - beta2) * grads[grad_key]**2

          velocities_hat[param_name] = velocities[param_name] / (1 - beta2)

          # Update the parameters using the updated velocities
          updated_parameters[param_name] -= (learning_rate/np.sqrt(velocities[param_name] + epsilon)) * (beta1 * moment_hat[param_name] + (((1 - beta1) * grads[grad_key])/ (1 - beta1)))

      return updated_parameters


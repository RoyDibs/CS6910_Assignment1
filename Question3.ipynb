{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "* sgd\n",
        "* momentum based gradient descent\n",
        "* nesterov accelerated gradient descent\n",
        "* rmsprop\n",
        "* adam\n",
        "* nadam\n"
      ],
      "metadata": {
        "id": "21_exgUBrCUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import data"
      ],
      "metadata": {
        "id": "SyX6TSsjFw7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Data and preprocesing\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "class_num = 10\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(class_num):\n",
        "\n",
        "    idx = np.where(i == y_train)[0][0]\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[idx], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_labels[y_train[idx]])\n",
        "plt.show()\n",
        "\n",
        "def train_val_split(X, y, val_size, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    num_samples = X.shape[0]\n",
        "    num_val_samples = int(num_samples * val_size)\n",
        "\n",
        "    # Shuffle indices\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Split indices into train and test sets\n",
        "    val_indices = indices[:num_val_samples]\n",
        "    train_indices = indices[num_val_samples:]\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val = X[train_indices], X[val_indices]\n",
        "    y_train, y_val = y[train_indices], y[val_indices]\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_val_split(x_train, y_train, val_size=0.1, random_state=2)\n",
        "print(\"Done!\")\n",
        "\n",
        "print(\"Size of Training data:\", x_train.shape)\n",
        "print(\"Size of Validation data:\", x_val.shape)\n",
        "\n",
        "x_train_scaled = x_train/225.0\n",
        "X_val_scaled = x_val/225\n",
        "X_test_scaled = x_test/225\n",
        "\n",
        "x_train_scaled = x_train_scaled.reshape(x_train_scaled.shape[0], x_train_scaled.shape[1]*x_train_scaled.shape[2]).T\n",
        "X_val_scaled = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1]*X_val_scaled.shape[2]).T\n",
        "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1]*X_test_scaled.shape[2]).T\n",
        "\n",
        "# One-hot encoding the labels\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    num_samples = len(labels)\n",
        "    one_hot_labels = np.zeros((num_classes, num_samples))\n",
        "    for i, label in enumerate(labels):\n",
        "        one_hot_labels[label, i] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "y_train_one_hot = one_hot_encode(y_train, class_num)\n",
        "y_val_one_hot = one_hot_encode(y_val, class_num)\n",
        "y_test_one_hot = one_hot_encode(y_test, class_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "ssMjdRUHFwpB",
        "outputId": "5b8d3d34-8a4e-47b4-8dae-c969e0659620"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "Size of Training data: (54000, 28, 28)\n",
            "Size of Validation data: (6000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise weights"
      ],
      "metadata": {
        "id": "Y_7GowOUGBtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InitialiseParams:\n",
        "    def __init__(self, num_layers, hidden_size, input_features, num_classes):\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_features = input_features\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def initialise_params(self):\n",
        "        # Create a list of layer dimensions [input_features, hidden_size, ..., hidden_size, num_classes]\n",
        "        layer_dim = [self.input_features]\n",
        "        for _ in range(self.num_layers):\n",
        "            layer_dim.append(self.hidden_size)\n",
        "        layer_dim.append(self.num_classes)\n",
        "        return layer_dim\n",
        "\n",
        "    def random_ini(self, layer_dim):\n",
        "        parameters = {}\n",
        "        np.random.seed(0)\n",
        "        num_layers = len(layer_dim)\n",
        "        for l in range(1, num_layers):\n",
        "            parameters[f'W{l}'] = np.random.randn(layer_dim[l], layer_dim[l-1]) * 0.01 # 0.1 doesn't work\n",
        "            parameters[f'b{l}'] = np.zeros((layer_dim[l], 1))\n",
        "        return parameters\n",
        "\n",
        "    def xavier_ini(self, layer_dim):\n",
        "        parameters = {}\n",
        "        np.random.seed(0)\n",
        "        num_layers = len(layer_dim)\n",
        "        for l in range(1, num_layers):\n",
        "            parameters[f'W{l}'] = np.random.randn(layer_dim[l], layer_dim[l-1]) * np.sqrt(1 / layer_dim[l-1])\n",
        "            parameters[f'b{l}'] = np.zeros((layer_dim[l], 1))\n",
        "        return parameters\n"
      ],
      "metadata": {
        "id": "_qnX8JUWGVOz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation functions"
      ],
      "metadata": {
        "id": "VI8QZ3aPGaj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def value(self, x):\n",
        "      return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def grad(self, x):\n",
        "      return self.value(x)*(1 - self.value(x))\n",
        "\n",
        "class tanh():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def value(self, x):\n",
        "      return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "    def grad(self, x):\n",
        "      return 1 - self.value(x)**2\n",
        "\n",
        "class identity():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def value(self, x):\n",
        "      return x\n",
        "\n",
        "    def grad(self, x):\n",
        "      return 1\n",
        "\n",
        "class ReLU():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def value(self, x):\n",
        "      return np.maximum(0, x)\n",
        "\n",
        "    def grad(self, x):\n",
        "      return np.where(x > 0, 1, 0)\n",
        "\n",
        "class softmax():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def value(self, x):\n",
        "        x_stable = x - np.max(x, axis=0, keepdims=True)\n",
        "        exp_values = np.exp(x_stable)\n",
        "        softmax_values = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "        return softmax_values\n",
        "    # def value(self, x):\n",
        "    #   return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    def grad(self, x):\n",
        "      return self.value(x) * (1 - self.value(x))"
      ],
      "metadata": {
        "id": "cpADR5cjGh2m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation"
      ],
      "metadata": {
        "id": "BczgYy2bGpHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ForwardPropagation():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_propagation(self, X, layer_dims, parameters, activation_fn):\n",
        "        self.parameters = parameters\n",
        "        self.activation_fn = activation_fn\n",
        "        caches = []\n",
        "        A = X\n",
        "        L = len(layer_dims) - 1\n",
        "\n",
        "        for l in range(1, L):\n",
        "            A_prev = A\n",
        "            W = self.parameters[f'W{l}']\n",
        "            b = self.parameters[f'b{l}']\n",
        "            Z = np.dot(W, A_prev) + b\n",
        "            activation = self.activation_fn.value(Z)\n",
        "            cache = (A_prev, W, b, Z, self.activation_fn)  # Include the activation function object in the cache\n",
        "            caches.append(cache)\n",
        "            A = activation\n",
        "\n",
        "        # Output layer\n",
        "        AL_prev = A\n",
        "        WL = self.parameters[f'W{L}']\n",
        "        bL = self.parameters[f'b{L}']\n",
        "        ZL = np.dot(WL, AL_prev) + bL\n",
        "        output = softmax().value(ZL)  # Using softmax activation for output layer\n",
        "\n",
        "        cache = (AL_prev, WL, bL, ZL, softmax())  # Include softmax activation function in the cache\n",
        "        caches.append(cache)\n",
        "\n",
        "        return output, caches"
      ],
      "metadata": {
        "id": "Q0cTNAdgGuLR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Propagation"
      ],
      "metadata": {
        "id": "JsYVQpxuGzCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BackPropagation:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy_loss(Y, Y_pred, parameters, alpha):\n",
        "        m = Y.shape[1]\n",
        "        cross_entropy_cost = -1/m * np.sum(Y * np.log(Y_pred + 1e-6))\n",
        "        regularization_term = 0\n",
        "        for key in parameters:\n",
        "            if key.startswith('W'):\n",
        "                regularization_term += np.sum(np.square(parameters[key]))\n",
        "        l2_regularization_cost = (alpha / (2 * m)) * regularization_term\n",
        "        return cross_entropy_cost + l2_regularization_cost\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_squared_error_loss(Y, Y_pred, parameters, alpha):\n",
        "        m = Y.shape[1]\n",
        "        mse_cost = 1/(2*m) * np.sum((Y - Y_pred)**2)\n",
        "        regularization_term = 0\n",
        "        for key in parameters:\n",
        "            if key.startswith('W'):\n",
        "                regularization_term += np.sum(np.square(parameters[key]))\n",
        "        l2_regularization_cost = (alpha / (2 * m)) * regularization_term\n",
        "        return mse_cost + l2_regularization_cost\n",
        "\n",
        "    def backward_propagation(self, Y, Y_pred, caches, loss_function, parameters, alpha):\n",
        "        grads = {}\n",
        "        m = Y.shape[1]\n",
        "        L = len(caches)\n",
        "\n",
        "        # Derivative of the loss function\n",
        "        if loss_function == 'cross_entropy':\n",
        "            dAL = - Y / (Y_pred + 1e-6)\n",
        "        elif loss_function == 'mse':\n",
        "            dAL = - (Y - Y_pred)\n",
        "\n",
        "        # Output layer gradients\n",
        "        current_cache = caches[L - 1]\n",
        "        activation_fn = current_cache[-1].__class__.__name__  # Extract activation function class name from cache\n",
        "        grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.linear_activation_backward(\n",
        "            dAL, current_cache, activation_fn, parameters, alpha)\n",
        "\n",
        "        # Hidden layers gradients\n",
        "        for l in reversed(range(L - 1)):\n",
        "            current_cache = caches[l]\n",
        "            activation_fn = current_cache[-1].__class__.__name__  # Extract activation function class name from cache\n",
        "            dA_prev_temp, dW_temp, db_temp = self.linear_activation_backward(\n",
        "                grads[\"dA\" + str(l + 2)], current_cache, activation_fn, parameters, alpha)\n",
        "            grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "            grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "        return grads\n",
        "\n",
        "    @staticmethod\n",
        "    def linear_activation_backward(dA, cache, activation_fn, parameters, alpha):\n",
        "        A_prev, W, b, Z = cache[:-1]  # Extract all but the last element from cache\n",
        "\n",
        "        if activation_fn == Sigmoid().__class__.__name__:\n",
        "            activation = Sigmoid()\n",
        "            dZ = dA * activation.grad(Z)\n",
        "        elif activation_fn == tanh().__class__.__name__:\n",
        "            activation = tanh()\n",
        "            dZ = dA * activation.grad(Z)\n",
        "        elif activation_fn == ReLU().__class__.__name__:\n",
        "            activation = ReLU()\n",
        "            dZ = dA * activation.grad(Z)\n",
        "        elif activation_fn == softmax().__class__.__name__:\n",
        "            activation = softmax()\n",
        "            dZ = dA * activation.grad(Z)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "        m = A_prev.shape[1]\n",
        "        dW = 1/m * np.dot(dZ, A_prev.T) + (alpha / m) * W  # Added L2 regularization term\n",
        "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "        return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "ozA_zkIdG2TE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimiser"
      ],
      "metadata": {
        "id": "mNnaHE-9G7EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimiser:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def stochastic_gradient_descent(self, parameters, grads, learning_rate, alpha):\n",
        "        updated_parameters = parameters.copy()\n",
        "\n",
        "        for param_name in parameters.keys():\n",
        "            grad_key = 'd' + param_name\n",
        "\n",
        "            # Update parameters with L2 regularization\n",
        "            updated_parameters[param_name] -= learning_rate * (grads[grad_key] + alpha * parameters[param_name])\n",
        "\n",
        "        return updated_parameters\n",
        "\n",
        "\n",
        "    def momentum(self, parameters, grads, velocities, beta, learning_rate, alpha):\n",
        "        updated_parameters = parameters.copy()\n",
        "\n",
        "        for param_name in parameters.keys():\n",
        "            grad_key = 'd' + param_name\n",
        "\n",
        "            # Update velocities\n",
        "            velocities[param_name] = beta * velocities[param_name] + learning_rate * (grads[grad_key] + alpha * parameters[param_name])\n",
        "\n",
        "            # Update parameters using velocities\n",
        "            updated_parameters[param_name] -= velocities[param_name]\n",
        "\n",
        "        return updated_parameters\n",
        "\n",
        "\n",
        "    def nestrov(self, X_batch, y_batch, parameters, velocities, beta, learning_rate, loss_function, activation_fn, alpha, layer_dims):\n",
        "        updated_parameters = parameters.copy()  # Make a copy of the original parameters\n",
        "        future_parameters = {}\n",
        "\n",
        "        for param_name in parameters.keys():\n",
        "            grad_key = 'd' + param_name\n",
        "            # Update velocities based on gradients evaluated at the approximate future position\n",
        "            future_parameters[param_name] = parameters[param_name] - beta * velocities[param_name]\n",
        "\n",
        "        forward_propagator = ForwardPropagation()\n",
        "        backpropagator = BackPropagation()\n",
        "\n",
        "        # Forward propagation\n",
        "        AL, caches = forward_propagator.forward_propagation(X_batch, layer_dims, future_parameters, activation_fn)\n",
        "        # Backward propagation\n",
        "        grads = backpropagator.backward_propagation(y_batch, AL, caches, loss_function, parameters, alpha)\n",
        "\n",
        "        for param_name in parameters.keys():\n",
        "            grad_key = 'd' + param_name\n",
        "            future_grads = grads[grad_key]  # Gradients evaluated at the approximate future position\n",
        "            velocities[param_name] = beta * velocities[param_name] + learning_rate * (future_grads + alpha * parameters[param_name])\n",
        "            # Update parameters using updated velocities\n",
        "            updated_parameters[param_name] -= velocities[param_name]\n",
        "\n",
        "        return updated_parameters\n",
        "\n",
        "\n",
        "    def rmsprop(self, parameters, grads, velocities, beta, learning_rate, alpha, epsilon):\n",
        "        updated_parameters = parameters.copy()\n",
        "\n",
        "        for param_name in parameters.keys():\n",
        "            grad_key = 'd' + param_name\n",
        "\n",
        "            # Update velocities\n",
        "            velocities[param_name] = beta * velocities[param_name] + (1 - beta) * np.square(grads[grad_key] + alpha * parameters[param_name])\n",
        "\n",
        "            # Update parameters using updated velocities\n",
        "            updated_parameters[param_name] -= (learning_rate/np.sqrt(velocities[param_name] + epsilon)) * (grads[grad_key] + alpha * parameters[param_name])\n",
        "\n",
        "        return updated_parameters\n",
        "\n",
        "\n",
        "    def adam(self, parameters, grads, moment, velocities, beta1, beta2, learning_rate, alpha, epsilon):\n",
        "        updated_parameters = parameters.copy()\n",
        "        moment_hat = {}\n",
        "        velocities_hat = {}\n",
        "\n",
        "        for param_name in parameters.keys():\n",
        "            grad_key = 'd' + param_name\n",
        "\n",
        "            # Update moment\n",
        "            moment[param_name] = beta1 * moment[param_name] + (1 - beta1) * (grads[grad_key] + alpha * parameters[param_name])\n",
        "\n",
        "            moment_hat[param_name] = moment[param_name] / (1 - beta1)\n",
        "\n",
        "            # Update velocities\n",
        "            velocities[param_name] = beta2 * velocities[param_name] + (1 - beta2) * (grads[grad_key] + alpha * parameters[param_name])**2\n",
        "\n",
        "            velocities_hat[param_name] = velocities[param_name] / (1 - beta2)\n",
        "\n",
        "            # Update parameters using updated velocities\n",
        "            updated_parameters[param_name] -= (learning_rate / (np.sqrt(velocities_hat[param_name]) + epsilon)) * moment_hat[param_name]\n",
        "\n",
        "        return updated_parameters\n",
        "\n",
        "\n",
        "\n",
        "    def nadam(self, parameters, grads, moment, velocities, beta1, beta2, learning_rate, alpha, epsilon):\n",
        "        updated_parameters = parameters.copy()\n",
        "        moment_hat = {}\n",
        "        velocities_hat = {}\n",
        "\n",
        "        for param_name in parameters.keys():\n",
        "            grad_key = 'd' + param_name\n",
        "\n",
        "            # Update moment\n",
        "            moment[param_name] = beta1 * moment[param_name] + (1 - beta1) * grads[grad_key]\n",
        "\n",
        "            # Update velocities\n",
        "            velocities[param_name] = beta2 * velocities[param_name] + (1 - beta2) * grads[grad_key]**2\n",
        "\n",
        "            # Bias correction\n",
        "            moment_hat[param_name] = moment[param_name] / (1 - beta1)\n",
        "            velocities_hat[param_name] = velocities[param_name] / (1 - beta2)\n",
        "\n",
        "            # Update parameters using Nesterov Adam update rule\n",
        "            updated_parameters[param_name] -= (learning_rate / (np.sqrt(velocities_hat[param_name]) + epsilon)) * (\n",
        "                        beta1 * moment_hat[param_name] + ((1 - beta1) * grads[grad_key]) / (1 - beta1))\n",
        "\n",
        "        return updated_parameters"
      ],
      "metadata": {
        "id": "CfgEonkFG_PI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train file"
      ],
      "metadata": {
        "id": "FjaElWBgHWi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def train(X_train, x_val, y_train, y_val, parameters_random, activation_fn, layer_dims, forward_propagator, backpropagator, optimiser, optimizer_type, loss_function, max_epoch, batch_size, learning_rate, beta, beta1, beta2, alpha):\n",
        "    num_samples = X_train.shape[1]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    # Get initial parameters from forward propagator\n",
        "    parameters = parameters_random\n",
        "\n",
        "    # Initialize velocities and moment\n",
        "    velocities = {param_name: np.zeros_like(param) for param_name, param in parameters.items()}\n",
        "    moment = {param_name: np.zeros_like(param) for param_name, param in parameters.items()} # for adam\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "\n",
        "            X_batch = X_train[:, start_idx:end_idx]\n",
        "            y_batch = y_train[:, start_idx:end_idx]\n",
        "\n",
        "            # Forward propagation\n",
        "            AL, caches = forward_propagator.forward_propagation(X_batch, layer_dims, parameters, activation_fn)\n",
        "\n",
        "            # Compute loss\n",
        "            if loss_function == 'cross_entropy':\n",
        "                cost = backpropagator.cross_entropy_loss(y_batch, AL, parameters, alpha)\n",
        "            elif loss_function == 'mse':\n",
        "                cost = backpropagator.mean_squared_error_loss(y_batch, AL, parameters, alpha)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported loss function\")\n",
        "\n",
        "            # Backward propagation\n",
        "            grads = backpropagator.backward_propagation(y_batch, AL, caches, loss_function, parameters, alpha)\n",
        "\n",
        "            # Update parameters using the specified optimizer\n",
        "            if optimizer_type == 'sgd':\n",
        "                parameters = optimiser.stochastic_gradient_descent(parameters, grads, learning_rate, alpha)\n",
        "            elif optimizer_type == 'momentum':\n",
        "                parameters = optimiser.momentum(parameters, grads, velocities, beta, learning_rate, alpha)\n",
        "            elif optimizer_type == 'nestrov':\n",
        "                parameters = optimiser.nestrov(X_batch, y_batch, parameters, velocities, beta, learning_rate, loss_function, activation_fn, alpha)\n",
        "            elif optimizer_type == 'rmsprop':\n",
        "                parameters = optimiser.rmsprop(parameters, grads, velocities, beta, learning_rate, alpha)\n",
        "            elif optimizer_type == 'adam':\n",
        "                parameters = optimiser.adam(parameters, grads, moment, velocities, beta1, beta2, learning_rate, alpha)\n",
        "            elif optimizer_type == 'nadam':\n",
        "                parameters = optimiser.nadam(parameters, grads, moment, velocities, beta1, beta2, learning_rate, alpha, epsilon=1e-6)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported optimizer type\")\n",
        "\n",
        "            # validation\n",
        "            AL_val, caches_val = forward_propagator.forward_propagation(x_val, layer_dims, parameters, activation_fn)\n",
        "\n",
        "            # Compute loss\n",
        "            if loss_function == 'cross_entropy':\n",
        "                cost_val = backpropagator.cross_entropy_loss(y_val, AL_val, parameters, alpha)\n",
        "            elif loss_function == 'mse':\n",
        "                cost_val = backpropagator.mean_squared_error_loss(y_val, AL_val, parameters, alpha)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported loss function\")\n",
        "\n",
        "            if batch_idx % batch_size == 0:\n",
        "                print(f\"Epoch {epoch+1}/{max_epoch}, Batch {batch_idx+1}/{num_batches}, Loss: {cost}, validation: {cost_val}\")\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "R1lYUPGzHYlk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main file"
      ],
      "metadata": {
        "id": "d6jvDgc1HDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Define the model\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "# Configuration dictionary for user inputs\n",
        "config = {\n",
        "    'num_layers': 3,\n",
        "    'hidden_size': 65,\n",
        "    'input_features': 28 * 28,\n",
        "    'num_classes': 10,\n",
        "    'optimizer_type': 'nadam',\n",
        "    'learning_rate': 0.001,\n",
        "    'beta': 0.9,\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.999,\n",
        "    'max_epoch': 5,\n",
        "    'batch_size': 34,\n",
        "    'loss_function': 'cross_entropy',\n",
        "    'initialisation_type': 'xavier',\n",
        "    'alpha': 0.005\n",
        "}\n",
        "\n",
        "# Initialise parameters\n",
        "initializer = InitialiseParams(config['num_layers'], config['hidden_size'], config['input_features'], config['num_classes'])\n",
        "layer_dims = initializer.initialise_params()\n",
        "if config['initialisation_type'] == 'random':\n",
        "    parameters_random = initializer.random_ini(layer_dims)\n",
        "elif config['initialisation_type'] == 'xavier':\n",
        "    parameters_random = initializer.xavier_ini(layer_dims)\n",
        "else:\n",
        "    raise ValueError(\"Invalid initialization type. Choose 'random' or 'xavier'.\")\n",
        "\n",
        "# Activation function\n",
        "activation_fn = tanh()\n",
        "\n",
        "# Create forward and backward propagators\n",
        "forward_propagator = ForwardPropagation()\n",
        "backpropagator = BackPropagation()\n",
        "\n",
        "# Compute gradients using backward propagation\n",
        "AL, caches = forward_propagator.forward_propagation(x_train_scaled, layer_dims, parameters_random, activation_fn)\n",
        "grads = backpropagator.backward_propagation(y_train_one_hot, AL, caches, config['loss_function'], parameters_random, alpha=config['alpha'])\n",
        "\n",
        "# Choose optimizer type and parameters\n",
        "optimizer_type = config['optimizer_type']\n",
        "learning_rate = config['learning_rate']\n",
        "beta = config['beta']\n",
        "beta1 = config['beta1']\n",
        "beta2 = config['beta2']\n",
        "optimiser = Optimiser()\n",
        "\n",
        "# Train the model\n",
        "print(\"Training starts : \")\n",
        "parameters_trained = train(x_train_scaled, X_val_scaled, y_train_one_hot, y_val_one_hot, parameters_random,\n",
        "                           activation_fn, layer_dims, forward_propagator, backpropagator, optimiser, optimizer_type,\n",
        "                           loss_function=config['loss_function'], max_epoch=config['max_epoch'], batch_size=config['batch_size'],\n",
        "                           learning_rate=learning_rate, beta=beta, beta1=beta1, beta2=beta2, alpha=config['alpha'])\n",
        "\n",
        "print(\"Training end\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Test the model\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "def predict(X_test, parameters_trained, forward_propagator, layer_dims, activation_fn):\n",
        "    AL, _ = forward_propagator.forward_propagation(X_test, layer_dims, parameters_trained, activation_fn)\n",
        "    predictions = np.argmax(AL, axis=0)\n",
        "    return predictions\n",
        "\n",
        "def calculate_accuracy(predictions, y_test):\n",
        "    correct_predictions = np.sum(predictions == y_test)\n",
        "    total_predictions = len(y_test.T)\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Test\n",
        "predictions = predict(X_test_scaled, parameters_trained, forward_propagator, layer_dims, activation_fn)\n",
        "accuracy = calculate_accuracy(predictions, y_test)\n",
        "print(\"Prediction accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOnJS7qlHFd_",
        "outputId": "afdee311-1e52-49e2-9102-b16ca3d117de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training starts : \n",
            "Epoch 1/5, Batch 1/1588, Loss: 2.3534852788778884, validation: 2.2742247282448327\n",
            "Epoch 1/5, Batch 35/1588, Loss: 1.8066818646420917, validation: 1.5324220131882735\n",
            "Epoch 1/5, Batch 69/1588, Loss: 1.20805698911943, validation: 1.2994366095533274\n",
            "Epoch 1/5, Batch 103/1588, Loss: 1.2840157170692301, validation: 1.212210044925763\n",
            "Epoch 1/5, Batch 137/1588, Loss: 1.4928031118982634, validation: 1.173633852675791\n",
            "Epoch 1/5, Batch 171/1588, Loss: 1.805531185598529, validation: 1.2309385053678235\n",
            "Epoch 1/5, Batch 205/1588, Loss: 1.0978504498787545, validation: 1.1656514410849708\n",
            "Epoch 1/5, Batch 239/1588, Loss: 1.1313946796051588, validation: 1.1481209775700558\n",
            "Epoch 1/5, Batch 273/1588, Loss: 1.0073450282725853, validation: 1.085666693719702\n",
            "Epoch 1/5, Batch 307/1588, Loss: 0.9072291589825309, validation: 1.0944157032839341\n",
            "Epoch 1/5, Batch 341/1588, Loss: 1.1225106365250608, validation: 1.1712055186952688\n",
            "Epoch 1/5, Batch 375/1588, Loss: 1.074156467149627, validation: 1.0907940127283462\n",
            "Epoch 1/5, Batch 409/1588, Loss: 1.0809733585795462, validation: 1.124734186607417\n",
            "Epoch 1/5, Batch 443/1588, Loss: 1.0316041294886718, validation: 1.0721581469742338\n",
            "Epoch 1/5, Batch 477/1588, Loss: 0.9336018237465198, validation: 1.0625830243329195\n",
            "Epoch 1/5, Batch 511/1588, Loss: 1.2337376784546246, validation: 1.0904175895968473\n",
            "Epoch 1/5, Batch 545/1588, Loss: 1.0993812957803777, validation: 1.069662074449125\n",
            "Epoch 1/5, Batch 579/1588, Loss: 1.043431076965047, validation: 1.0740580039440868\n",
            "Epoch 1/5, Batch 613/1588, Loss: 1.5014010396792563, validation: 1.1513566695997517\n",
            "Epoch 1/5, Batch 647/1588, Loss: 0.9489181963822853, validation: 1.0498204303290382\n",
            "Epoch 1/5, Batch 681/1588, Loss: 0.8922747931033949, validation: 1.0656346642579508\n",
            "Epoch 1/5, Batch 715/1588, Loss: 0.8948538901924262, validation: 1.077521135533835\n",
            "Epoch 1/5, Batch 749/1588, Loss: 1.1108004535180158, validation: 1.0823106147058814\n",
            "Epoch 1/5, Batch 783/1588, Loss: 0.8161765984531483, validation: 1.0569584985836578\n",
            "Epoch 1/5, Batch 817/1588, Loss: 1.2376981265697844, validation: 1.083521507057569\n",
            "Epoch 1/5, Batch 851/1588, Loss: 0.9434729666540962, validation: 1.0753161757868253\n",
            "Epoch 1/5, Batch 885/1588, Loss: 1.1472441468563344, validation: 1.0641928330633565\n",
            "Epoch 1/5, Batch 919/1588, Loss: 1.0255220921253259, validation: 1.0899881895279575\n",
            "Epoch 1/5, Batch 953/1588, Loss: 1.2489234176822817, validation: 1.0368248631317634\n",
            "Epoch 1/5, Batch 987/1588, Loss: 0.941511762433668, validation: 1.0118208506068362\n",
            "Epoch 1/5, Batch 1021/1588, Loss: 1.21138480761654, validation: 1.0742546083890119\n",
            "Epoch 1/5, Batch 1055/1588, Loss: 1.4110916030828626, validation: 1.0419259645333276\n",
            "Epoch 1/5, Batch 1089/1588, Loss: 1.322134585468634, validation: 1.1150056484121245\n",
            "Epoch 1/5, Batch 1123/1588, Loss: 1.2582677766078618, validation: 1.0399052360050904\n",
            "Epoch 1/5, Batch 1157/1588, Loss: 1.1754322809151285, validation: 1.051330116876901\n",
            "Epoch 1/5, Batch 1191/1588, Loss: 1.352499718484905, validation: 1.0333399116070694\n",
            "Epoch 1/5, Batch 1225/1588, Loss: 0.8104337296386219, validation: 1.0583171417086017\n",
            "Epoch 1/5, Batch 1259/1588, Loss: 1.8730588957783298, validation: 1.001744645265572\n",
            "Epoch 1/5, Batch 1293/1588, Loss: 1.0383614935676402, validation: 1.0822913478292777\n",
            "Epoch 1/5, Batch 1327/1588, Loss: 1.1377781005611867, validation: 1.2088111724567654\n",
            "Epoch 1/5, Batch 1361/1588, Loss: 1.5954898473788786, validation: 1.0054059344713524\n",
            "Epoch 1/5, Batch 1395/1588, Loss: 1.6496819948360133, validation: 1.008932294539623\n",
            "Epoch 1/5, Batch 1429/1588, Loss: 1.0610826511273017, validation: 1.108426681217777\n",
            "Epoch 1/5, Batch 1463/1588, Loss: 1.0196190407420536, validation: 1.0779530446631653\n",
            "Epoch 1/5, Batch 1497/1588, Loss: 1.0567433315301364, validation: 1.0314834991314727\n",
            "Epoch 1/5, Batch 1531/1588, Loss: 1.4673643253379136, validation: 1.0821268557596875\n",
            "Epoch 1/5, Batch 1565/1588, Loss: 1.4113482403680038, validation: 1.0584465762925803\n",
            "Epoch 2/5, Batch 1/1588, Loss: 1.3331630074802645, validation: 1.1185184206363146\n",
            "Epoch 2/5, Batch 35/1588, Loss: 1.477758357541723, validation: 1.0790601224246883\n",
            "Epoch 2/5, Batch 69/1588, Loss: 0.8702852119430193, validation: 1.0268673308834757\n",
            "Epoch 2/5, Batch 103/1588, Loss: 0.8938101521771763, validation: 1.0147268948384651\n",
            "Epoch 2/5, Batch 137/1588, Loss: 1.4625677021447103, validation: 1.042543080703566\n",
            "Epoch 2/5, Batch 171/1588, Loss: 1.2442165620355985, validation: 1.0319548643935867\n",
            "Epoch 2/5, Batch 205/1588, Loss: 0.9793033696562958, validation: 0.9840309352438081\n",
            "Epoch 2/5, Batch 239/1588, Loss: 0.7816242228887463, validation: 1.0477646346373468\n",
            "Epoch 2/5, Batch 273/1588, Loss: 0.8665442298003598, validation: 1.0422178573565604\n",
            "Epoch 2/5, Batch 307/1588, Loss: 0.7537771686116685, validation: 1.0925207282597948\n",
            "Epoch 2/5, Batch 341/1588, Loss: 1.0684459012569625, validation: 1.066616962237937\n",
            "Epoch 2/5, Batch 375/1588, Loss: 1.0601549421981735, validation: 1.1031910459200553\n",
            "Epoch 2/5, Batch 409/1588, Loss: 1.0657459891001946, validation: 1.0818672289681441\n",
            "Epoch 2/5, Batch 443/1588, Loss: 0.8411758186819157, validation: 0.9984025734342304\n",
            "Epoch 2/5, Batch 477/1588, Loss: 0.8638246348656514, validation: 1.0759911953910732\n",
            "Epoch 2/5, Batch 511/1588, Loss: 1.2867675569314982, validation: 1.06046982870823\n",
            "Epoch 2/5, Batch 545/1588, Loss: 1.3278942873452229, validation: 1.1074323429072177\n",
            "Epoch 2/5, Batch 579/1588, Loss: 1.2670911314622626, validation: 1.082317334522097\n",
            "Epoch 2/5, Batch 613/1588, Loss: 1.2752770163273155, validation: 1.091789006623674\n",
            "Epoch 2/5, Batch 647/1588, Loss: 0.9301041188533149, validation: 1.0482327668908968\n",
            "Epoch 2/5, Batch 681/1588, Loss: 0.7818227488528152, validation: 1.0703642071317199\n",
            "Epoch 2/5, Batch 715/1588, Loss: 0.72551933239304, validation: 1.0928423596969952\n",
            "Epoch 2/5, Batch 749/1588, Loss: 1.3287271978524693, validation: 1.1417569659748272\n",
            "Epoch 2/5, Batch 783/1588, Loss: 0.8330240530666931, validation: 1.1628583104991381\n",
            "Epoch 2/5, Batch 817/1588, Loss: 1.4307370806986834, validation: 1.1335500325715229\n",
            "Epoch 2/5, Batch 851/1588, Loss: 0.9033823071023864, validation: 1.0861118491121542\n",
            "Epoch 2/5, Batch 885/1588, Loss: 1.396713536969073, validation: 1.1174644192813776\n",
            "Epoch 2/5, Batch 919/1588, Loss: 0.9643155352648316, validation: 1.138274401366187\n",
            "Epoch 2/5, Batch 953/1588, Loss: 1.0695798417892302, validation: 1.0526888028714876\n",
            "Epoch 2/5, Batch 987/1588, Loss: 0.9714987198096164, validation: 1.0582228205110635\n",
            "Epoch 2/5, Batch 1021/1588, Loss: 1.017457545179056, validation: 1.0237328245035566\n",
            "Epoch 2/5, Batch 1055/1588, Loss: 1.04936391636893, validation: 1.1052448164845001\n",
            "Epoch 2/5, Batch 1089/1588, Loss: 1.2473813612697895, validation: 1.1059635999651862\n",
            "Epoch 2/5, Batch 1123/1588, Loss: 1.0829539725430253, validation: 1.0740631956013686\n",
            "Epoch 2/5, Batch 1157/1588, Loss: 1.1639734068425767, validation: 1.0610832126407461\n",
            "Epoch 2/5, Batch 1191/1588, Loss: 2.1120287216729547, validation: 1.1626430026478105\n",
            "Epoch 2/5, Batch 1225/1588, Loss: 0.838384131300866, validation: 1.0654719786887208\n",
            "Epoch 2/5, Batch 1259/1588, Loss: 2.1579074174473756, validation: 1.1177324757027125\n",
            "Epoch 2/5, Batch 1293/1588, Loss: 0.9972442952424566, validation: 1.0923948852197831\n",
            "Epoch 2/5, Batch 1327/1588, Loss: 0.8829292191555883, validation: 1.091098755130272\n",
            "Epoch 2/5, Batch 1361/1588, Loss: 1.836194577699271, validation: 1.1619047427809561\n",
            "Epoch 2/5, Batch 1395/1588, Loss: 1.835453482784708, validation: 1.0268711501628491\n",
            "Epoch 2/5, Batch 1429/1588, Loss: 1.09459708659401, validation: 1.0914703750442107\n",
            "Epoch 2/5, Batch 1463/1588, Loss: 0.9150911604708495, validation: 1.18893280874675\n",
            "Epoch 2/5, Batch 1497/1588, Loss: 1.1844143069461102, validation: 1.0612776219173465\n",
            "Epoch 2/5, Batch 1531/1588, Loss: 1.6170040221447175, validation: 1.1069452142684175\n",
            "Epoch 2/5, Batch 1565/1588, Loss: 1.5322223856045323, validation: 1.1840570976343006\n",
            "Epoch 3/5, Batch 1/1588, Loss: 1.5207876049998554, validation: 1.1527855891950174\n",
            "Epoch 3/5, Batch 35/1588, Loss: 1.4790286324895532, validation: 1.129402783581979\n",
            "Epoch 3/5, Batch 69/1588, Loss: 0.9834209213075312, validation: 1.113368172406598\n",
            "Epoch 3/5, Batch 103/1588, Loss: 0.874548386120562, validation: 1.0471500192468923\n",
            "Epoch 3/5, Batch 137/1588, Loss: 1.6086195802593486, validation: 1.1236044117852697\n",
            "Epoch 3/5, Batch 171/1588, Loss: 1.2441563032842835, validation: 1.0953848832916626\n",
            "Epoch 3/5, Batch 205/1588, Loss: 1.1319082923378843, validation: 1.080270244403325\n",
            "Epoch 3/5, Batch 239/1588, Loss: 0.7546176019578592, validation: 1.0828222926356288\n",
            "Epoch 3/5, Batch 273/1588, Loss: 0.7749730263568831, validation: 1.1380480660667247\n",
            "Epoch 3/5, Batch 307/1588, Loss: 0.7341382821162147, validation: 1.1806810060848303\n",
            "Epoch 3/5, Batch 341/1588, Loss: 1.226010718312295, validation: 1.1439004680724876\n",
            "Epoch 3/5, Batch 375/1588, Loss: 1.1695263737820145, validation: 1.0496549501999104\n",
            "Epoch 3/5, Batch 409/1588, Loss: 1.1804852383572557, validation: 1.1018848701543302\n",
            "Epoch 3/5, Batch 443/1588, Loss: 1.251390277306697, validation: 1.0637992850945137\n",
            "Epoch 3/5, Batch 477/1588, Loss: 0.8792768485319021, validation: 1.1299473477659387\n",
            "Epoch 3/5, Batch 511/1588, Loss: 1.3793086055785901, validation: 1.106149237436701\n",
            "Epoch 3/5, Batch 545/1588, Loss: 1.629347326991503, validation: 1.1336711022197203\n",
            "Epoch 3/5, Batch 579/1588, Loss: 1.1984712775120694, validation: 1.1412591866766764\n",
            "Epoch 3/5, Batch 613/1588, Loss: 1.2040308583596673, validation: 1.1069100843079247\n",
            "Epoch 3/5, Batch 647/1588, Loss: 0.8922805489803457, validation: 1.068769591351206\n",
            "Epoch 3/5, Batch 681/1588, Loss: 0.7969996141407961, validation: 1.063821122646261\n",
            "Epoch 3/5, Batch 715/1588, Loss: 0.7290504748307383, validation: 1.1424251486910004\n",
            "Epoch 3/5, Batch 749/1588, Loss: 1.5122190538956317, validation: 1.210004811563615\n",
            "Epoch 3/5, Batch 783/1588, Loss: 0.8794239001997207, validation: 1.1673709877367053\n",
            "Epoch 3/5, Batch 817/1588, Loss: 1.332961675180365, validation: 1.1509282551474538\n",
            "Epoch 3/5, Batch 851/1588, Loss: 0.9599533228684815, validation: 1.1481693269718358\n",
            "Epoch 3/5, Batch 885/1588, Loss: 1.5509667400477076, validation: 1.1006193674385762\n",
            "Epoch 3/5, Batch 919/1588, Loss: 1.0299648839519118, validation: 1.1451349309485404\n",
            "Epoch 3/5, Batch 953/1588, Loss: 0.9349829479238887, validation: 1.0996490909892036\n",
            "Epoch 3/5, Batch 987/1588, Loss: 1.086732737961343, validation: 1.183594237993683\n",
            "Epoch 3/5, Batch 1021/1588, Loss: 1.180686035370372, validation: 1.1220509693263492\n",
            "Epoch 3/5, Batch 1055/1588, Loss: 1.6074301858920605, validation: 1.130552383335588\n",
            "Epoch 3/5, Batch 1089/1588, Loss: 1.2625911186065715, validation: 1.2432607959875335\n",
            "Epoch 3/5, Batch 1123/1588, Loss: 1.4880182582881685, validation: 1.1295899719283333\n",
            "Epoch 3/5, Batch 1157/1588, Loss: 1.2329552636193388, validation: 1.1278891026028228\n",
            "Epoch 3/5, Batch 1191/1588, Loss: 2.0641877254628107, validation: 1.1961417438763553\n",
            "Epoch 3/5, Batch 1225/1588, Loss: 0.9376068118833332, validation: 1.09468773448928\n",
            "Epoch 3/5, Batch 1259/1588, Loss: 2.2329527225195305, validation: 1.2283646914337962\n",
            "Epoch 3/5, Batch 1293/1588, Loss: 0.9868758486907867, validation: 1.2369575169558285\n",
            "Epoch 3/5, Batch 1327/1588, Loss: 0.9081289838795145, validation: 1.1417364565028811\n",
            "Epoch 3/5, Batch 1361/1588, Loss: 2.0626606903628977, validation: 1.214679260197362\n",
            "Epoch 3/5, Batch 1395/1588, Loss: 1.9612307588026199, validation: 1.133158809088767\n",
            "Epoch 3/5, Batch 1429/1588, Loss: 1.1330516252460545, validation: 1.2423760596058022\n",
            "Epoch 3/5, Batch 1463/1588, Loss: 0.984059144516843, validation: 1.187905150803335\n",
            "Epoch 3/5, Batch 1497/1588, Loss: 1.2752321355389689, validation: 1.1581467560402565\n",
            "Epoch 3/5, Batch 1531/1588, Loss: 1.7969348220456935, validation: 1.2716343146721356\n",
            "Epoch 3/5, Batch 1565/1588, Loss: 1.6696481319052416, validation: 1.174796809142773\n",
            "Epoch 4/5, Batch 1/1588, Loss: 1.7631911153433264, validation: 1.2448061532458397\n",
            "Epoch 4/5, Batch 35/1588, Loss: 1.5300340522596882, validation: 1.2731889144391713\n",
            "Epoch 4/5, Batch 69/1588, Loss: 1.0437223213104674, validation: 1.0890214890292085\n",
            "Epoch 4/5, Batch 103/1588, Loss: 0.9467643231180592, validation: 1.0924527653725071\n",
            "Epoch 4/5, Batch 137/1588, Loss: 1.795115108298241, validation: 1.1660465533574134\n",
            "Epoch 4/5, Batch 171/1588, Loss: 1.257222654868544, validation: 1.1298418566952775\n",
            "Epoch 4/5, Batch 205/1588, Loss: 1.2137695460716615, validation: 1.0442558987912336\n",
            "Epoch 4/5, Batch 239/1588, Loss: 0.8579201675905167, validation: 1.1620567484294833\n",
            "Epoch 4/5, Batch 273/1588, Loss: 0.822239754539761, validation: 1.1290090773361503\n",
            "Epoch 4/5, Batch 307/1588, Loss: 0.7575243871277002, validation: 1.2176332717603053\n",
            "Epoch 4/5, Batch 341/1588, Loss: 1.4056786727497654, validation: 1.211135167382125\n",
            "Epoch 4/5, Batch 375/1588, Loss: 1.33314063961766, validation: 1.1564176959813497\n",
            "Epoch 4/5, Batch 409/1588, Loss: 1.2941203666671826, validation: 1.145410481980657\n",
            "Epoch 4/5, Batch 443/1588, Loss: 0.9175711257255921, validation: 1.0548811317521753\n",
            "Epoch 4/5, Batch 477/1588, Loss: 0.9534161109307143, validation: 1.178050427035917\n",
            "Epoch 4/5, Batch 511/1588, Loss: 1.5303997839601804, validation: 1.2223246074308447\n",
            "Epoch 4/5, Batch 545/1588, Loss: 1.833426650509622, validation: 1.1120997999792976\n",
            "Epoch 4/5, Batch 579/1588, Loss: 1.3546621546101827, validation: 1.253162330285262\n",
            "Epoch 4/5, Batch 613/1588, Loss: 1.5434113389425692, validation: 1.2352166076449396\n",
            "Epoch 4/5, Batch 647/1588, Loss: 0.9565408904170768, validation: 1.2130650574190225\n",
            "Epoch 4/5, Batch 681/1588, Loss: 0.8388304383131802, validation: 1.1959734661131343\n",
            "Epoch 4/5, Batch 715/1588, Loss: 0.764614106370316, validation: 1.2396453073392064\n",
            "Epoch 4/5, Batch 749/1588, Loss: 1.5522763044605952, validation: 1.1912622840765172\n",
            "Epoch 4/5, Batch 783/1588, Loss: 1.0059970425146494, validation: 1.2817854042337384\n",
            "Epoch 4/5, Batch 817/1588, Loss: 1.2823121287438877, validation: 1.1826741271218533\n",
            "Epoch 4/5, Batch 851/1588, Loss: 1.0212634295494167, validation: 1.2586684038145626\n",
            "Epoch 4/5, Batch 885/1588, Loss: 1.4543546621859011, validation: 1.099820530510016\n",
            "Epoch 4/5, Batch 919/1588, Loss: 1.09810896996208, validation: 1.1843536481410053\n",
            "Epoch 4/5, Batch 953/1588, Loss: 1.1340746076935948, validation: 1.1374076069650856\n",
            "Epoch 4/5, Batch 987/1588, Loss: 1.153914493356425, validation: 1.2109471382943247\n",
            "Epoch 4/5, Batch 1021/1588, Loss: 0.8372330238186818, validation: 1.077995682191021\n",
            "Epoch 4/5, Batch 1055/1588, Loss: 1.21031395609673, validation: 1.2257869591281518\n",
            "Epoch 4/5, Batch 1089/1588, Loss: 1.3232767595452632, validation: 1.194493417522477\n",
            "Epoch 4/5, Batch 1123/1588, Loss: 1.5248768868240674, validation: 1.1794060462038698\n",
            "Epoch 4/5, Batch 1157/1588, Loss: 1.5153295253030268, validation: 1.1519199238935007\n",
            "Epoch 4/5, Batch 1191/1588, Loss: 2.0877768248464, validation: 1.1860135149665052\n",
            "Epoch 4/5, Batch 1225/1588, Loss: 1.0680012706664666, validation: 1.1587696104664165\n",
            "Epoch 4/5, Batch 1259/1588, Loss: 2.2443877257823037, validation: 1.1601828470762223\n",
            "Epoch 4/5, Batch 1293/1588, Loss: 1.173157971023496, validation: 1.3098046792202553\n",
            "Epoch 4/5, Batch 1327/1588, Loss: 0.9707732365603565, validation: 1.247416422336219\n",
            "Epoch 4/5, Batch 1361/1588, Loss: 2.2282541782921372, validation: 1.316109993917804\n",
            "Epoch 4/5, Batch 1395/1588, Loss: 2.0896945493491232, validation: 1.2186053611441157\n",
            "Epoch 4/5, Batch 1429/1588, Loss: 1.309349784663702, validation: 1.373415349380059\n",
            "Epoch 4/5, Batch 1463/1588, Loss: 1.0841133540568404, validation: 1.176222702293689\n",
            "Epoch 4/5, Batch 1497/1588, Loss: 1.3340523919208134, validation: 1.1451900835461788\n",
            "Epoch 4/5, Batch 1531/1588, Loss: 1.9460054898765085, validation: 1.2367244510094957\n",
            "Epoch 4/5, Batch 1565/1588, Loss: 1.8816631738369107, validation: 1.2197009332092728\n",
            "Epoch 5/5, Batch 1/1588, Loss: 1.5935966905092305, validation: 1.1439893608454192\n",
            "Epoch 5/5, Batch 35/1588, Loss: 1.6054874484300299, validation: 1.3005458440771427\n",
            "Epoch 5/5, Batch 69/1588, Loss: 1.171978535661223, validation: 1.2160831651903048\n",
            "Epoch 5/5, Batch 103/1588, Loss: 1.0514334657323752, validation: 1.1535437583457016\n",
            "Epoch 5/5, Batch 137/1588, Loss: 1.8973322262447603, validation: 1.2288442244133833\n",
            "Epoch 5/5, Batch 171/1588, Loss: 1.695450453944076, validation: 1.2061177756829498\n",
            "Epoch 5/5, Batch 205/1588, Loss: 1.362706499511114, validation: 1.1421492933429946\n",
            "Epoch 5/5, Batch 239/1588, Loss: 0.9797184249211996, validation: 1.2333558980351753\n",
            "Epoch 5/5, Batch 273/1588, Loss: 0.8905103245130441, validation: 1.2039764657964338\n",
            "Epoch 5/5, Batch 307/1588, Loss: 0.8369461147811817, validation: 1.2284278582814088\n",
            "Epoch 5/5, Batch 341/1588, Loss: 1.5939380164836985, validation: 1.242695299427715\n",
            "Epoch 5/5, Batch 375/1588, Loss: 1.3498261514751044, validation: 1.2632529383336608\n",
            "Epoch 5/5, Batch 409/1588, Loss: 1.3534901819242082, validation: 1.1205896401707178\n",
            "Epoch 5/5, Batch 443/1588, Loss: 0.9990174841584164, validation: 1.0765534864498423\n",
            "Epoch 5/5, Batch 477/1588, Loss: 0.9875173840755074, validation: 1.1872352091738354\n",
            "Epoch 5/5, Batch 511/1588, Loss: 1.8248833966821234, validation: 1.3184119059271842\n",
            "Epoch 5/5, Batch 545/1588, Loss: 2.0100399770434207, validation: 1.2686553579688937\n",
            "Epoch 5/5, Batch 579/1588, Loss: 1.9844514029191735, validation: 1.2390859251960527\n",
            "Epoch 5/5, Batch 613/1588, Loss: 1.328316979889282, validation: 1.2425594601229395\n",
            "Epoch 5/5, Batch 647/1588, Loss: 0.9515743754538479, validation: 1.1228375337942818\n",
            "Epoch 5/5, Batch 681/1588, Loss: 0.9036707790565943, validation: 1.1925819131453346\n",
            "Epoch 5/5, Batch 715/1588, Loss: 0.8239805814881955, validation: 1.3440739918208937\n",
            "Epoch 5/5, Batch 749/1588, Loss: 1.62417640244721, validation: 1.2381081373547853\n",
            "Epoch 5/5, Batch 783/1588, Loss: 1.2237683125059031, validation: 1.3225542463577862\n",
            "Epoch 5/5, Batch 817/1588, Loss: 1.3715043398390592, validation: 1.2587665986626464\n",
            "Epoch 5/5, Batch 851/1588, Loss: 1.111546088298792, validation: 1.3210817485893556\n",
            "Epoch 5/5, Batch 885/1588, Loss: 1.2594406765333248, validation: 1.1669077789560152\n",
            "Epoch 5/5, Batch 919/1588, Loss: 1.264731525548997, validation: 1.2692848068228817\n",
            "Epoch 5/5, Batch 953/1588, Loss: 0.9445119937295217, validation: 1.1706146434117055\n",
            "Epoch 5/5, Batch 987/1588, Loss: 1.1065388301989578, validation: 1.1895556459962082\n",
            "Epoch 5/5, Batch 1021/1588, Loss: 0.9194571771615387, validation: 1.2076139409929807\n",
            "Epoch 5/5, Batch 1055/1588, Loss: 1.6495641999873083, validation: 1.204574834491026\n",
            "Epoch 5/5, Batch 1089/1588, Loss: 1.401998811429119, validation: 1.2953176744414054\n",
            "Epoch 5/5, Batch 1123/1588, Loss: 1.6342094633519493, validation: 1.2031584357188707\n",
            "Epoch 5/5, Batch 1157/1588, Loss: 1.5903022945962573, validation: 1.234557600205412\n",
            "Epoch 5/5, Batch 1191/1588, Loss: 2.0993530333014117, validation: 1.1679812157951792\n",
            "Epoch 5/5, Batch 1225/1588, Loss: 1.1827402521274644, validation: 1.171953983095531\n",
            "Epoch 5/5, Batch 1259/1588, Loss: 2.4650911475007393, validation: 1.3216793876696669\n",
            "Epoch 5/5, Batch 1293/1588, Loss: 1.1264888793338894, validation: 1.2669115501768418\n",
            "Epoch 5/5, Batch 1327/1588, Loss: 1.0524531951096587, validation: 1.4658624479612206\n",
            "Epoch 5/5, Batch 1361/1588, Loss: 2.5285973060851346, validation: 1.3434090152967446\n",
            "Epoch 5/5, Batch 1395/1588, Loss: 2.1879867834842965, validation: 1.1552929047085154\n",
            "Epoch 5/5, Batch 1429/1588, Loss: 1.3795076096493057, validation: 1.2849494835363442\n",
            "Epoch 5/5, Batch 1463/1588, Loss: 1.111426068143499, validation: 1.190617347259418\n",
            "Epoch 5/5, Batch 1497/1588, Loss: 1.412408059497769, validation: 1.1791731840702686\n",
            "Epoch 5/5, Batch 1531/1588, Loss: 2.116368824250781, validation: 1.2378156707048602\n",
            "Epoch 5/5, Batch 1565/1588, Loss: 2.0357977929448987, validation: 1.2430993385050897\n",
            "Training end\n",
            "\n",
            "\n",
            "Prediction accuracy: 63.36000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sit07xCSPVkf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
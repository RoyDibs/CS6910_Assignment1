# -*- coding: utf-8 -*-
"""Backward_propagation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mnUrAsDcnM5sYZ_HmiROnI7TBRrGk_Ch
"""

import numpy as np

class BackPropagation:
    def __init__(self):
        pass

    @staticmethod
    def cross_entropy_loss(Y, Y_pred):
        m = Y.shape[1]
        cost = -1/m * np.sum(Y * np.log(Y_pred))
        return np.squeeze(cost)

    @staticmethod
    def mean_squared_error_loss(Y, Y_pred):
        m = Y.shape[1]
        cost = 1/(2*m) * np.sum((Y - Y_pred)**2)
        return np.squeeze(cost)

    def backward_propagation(self, Y, Y_pred, caches, loss_function):
        grads = {}
        m = Y.shape[1]
        L = len(caches)

        # Derivative of the loss function
        if loss_function == 'cross_entropy':
            dAL = - (np.divide(Y, Y_pred) - np.divide(1 - Y, 1 - Y_pred))
        elif loss_function == 'mse':
            dAL = - (Y - Y_pred)

        # Output layer gradients
        current_cache = caches[L - 1]
        activation_fn = current_cache[-1].__class__.__name__
        grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = self.linear_activation_backward(
            dAL, current_cache, activation_fn)

        # Hidden layers gradients
        for l in reversed(range(L - 1)):
            current_cache = caches[l]
            activation_fn = current_cache[-1].__class__.__name__
            dA_prev_temp, dW_temp, db_temp = self.linear_activation_backward(
                grads["dA" + str(l + 2)], current_cache, activation_fn)
            grads["dA" + str(l + 1)] = dA_prev_temp
            grads["dW" + str(l + 1)] = dW_temp
            grads["db" + str(l + 1)] = db_temp

        return grads

    @staticmethod
    def linear_activation_backward(dA, cache, activation_fn):
        A_prev, W, b, Z = cache[:-1]  # Extract all the last element from cache except the last one

        if activation_fn == "Sigmoid":
            activation = Sigmoid()
            dZ = dA * activation.grad(Z)
        elif activation_fn == "tanh":
            activation = tanh()
            dZ = dA * activation.grad(Z)
        elif activation_fn == "ReLU":
            activation = ReLU()
            dZ = dA * activation.grad(Z)
        elif activation_fn == "softmax":
            activation = softmax()
            dZ = dA * activation.grad(Z)
        else:
            raise ValueError("Unsupported activation function")

        m = A_prev.shape[1]
        dW = 1/m * np.dot(dZ, A_prev.T)
        db = 1/m * np.sum(dZ, axis=1, keepdims=True)
        dA_prev = np.dot(W.T, dZ)

        return dA_prev, dW, db